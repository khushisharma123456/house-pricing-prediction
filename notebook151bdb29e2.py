# -*- coding: utf-8 -*-
"""notebook151bdb29e2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/khushisharma1210/notebook151bdb29e2.fd61dbe7-9583-4ecb-a2f2-0fd7c60de5d3.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20260204/auto/storage/goog4_request%26X-Goog-Date%3D20260204T210901Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D11c143310cd27e019f9d2d05c2c42c65a55f86cc2b83e7d5d33c4b03ea5b121440d98cb5bcb7c112db8848d78829aaac3f6eae09c35e7451dbe830619791a054474c4a4e2ce5a62549986897a0173badfd1378a88ed0e1e3834f618ef22a5eeb10b078f655e76501067790461fc9cd84be220963f05d2b99980fe6088b85017902f4cfc217b990e5e9eed884a01751e6f3f88ff98dde4b7f8ab00c31f85d284fae6ab7b7b75507907a42eaf13966e1fb5a0f13f7defb4558cda27f6f84476c5594e2f169720eae7b3fcedb242236b1993435fd8bfbdf73fa389979d675bf0d9034028546b4eb8ae4eb993c96414d9bcbdac5e91620c4911956b0aa32c216dccc
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

mlp_term_3_2025_kaggle_assignment_1_path = kagglehub.competition_download('mlp-term-3-2025-kaggle-assignment-1')

print('Data source import complete.')

# ==========================
# Step 1 — Import Libraries
# ==========================
print("Importing libraries...")
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from scipy.stats import skew # For skewness handling

# Model Selection & Metrics
from sklearn.model_selection import train_test_split, KFold # Using simple split + KFold for evaluation reference
from sklearn.metrics import mean_squared_error

# Preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer # Using SimpleImputer

# Models
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR # Added SVR
import lightgbm as lgb
from catboost import CatBoostRegressor
import xgboost as XGB # Added XGBoost for model variety

# Settings
warnings.filterwarnings('ignore')
pd.set_option('display.float_format', lambda x: '%.2f' % x) # Set float format
# Set plotting style
plt.style.use('seaborn-v0_8-darkgrid')
print("Libraries imported successfully.")

# ==========================
# Step 2 — Load Data
# ==========================
print("\nLoading data...")
try:
    # --- Define Paths ---
    # Adjust BASE_PATH if running locally after downloading from KaggleHub
    BASE_PATH = './mlp-term-3-2025-kaggle-assignment-1/'
    if not os.path.exists(BASE_PATH):
      # If running in Kaggle environment
      BASE_PATH = '/kaggle/input/mlp-term-3-2025-kaggle-assignment-1/'

    TRAIN_PATH = os.path.join(BASE_PATH, 'train.csv')
    TEST_PATH = os.path.join(BASE_PATH, 'test.csv')
    SUBMISSION_PATH = os.path.join(BASE_PATH, 'sample_submission.csv')

    # --- Load DataFrames ---
    train_df_orig = pd.read_csv(TRAIN_PATH)
    test_df_orig = pd.read_csv(TEST_PATH)
    sample_sub = pd.read_csv(SUBMISSION_PATH)

    # Make copies
    train_df = train_df_orig.copy()
    test_df = test_df_orig.copy()

    # Store IDs
    train_ids = train_df['id'] # Keep train IDs if needed
    test_ids = test_df['id']   # Store test IDs for submission
    print("Data loaded successfully.")
    print(f"Original Training data shape: {train_df.shape}")
    print(f"Original Test data shape: {test_df.shape}")

except FileNotFoundError:
    print("\n--- ! ERROR ! ---")
    print("Could not find data files. Please check input paths.")
    print("If running locally, ensure you have downloaded the data and placed it in the correct directory.")
except Exception as e:
    print(f"\n--- ! ERROR ! --- An error occurred during data loading: {e}")

# ==========================
# Step 3 — Initial Data Exploration
# ==========================
print("\n--- Initial Data Exploration ---")

# --- 3.1: Data Types (Rubric 1) ---
print("\n--- 3.1: Data Types ---")
print("Identifying data types of different columns:")
print(train_df.info())

# --- 3.2: Descriptive Statistics (Rubric 2) ---
print("\n--- 3.2: Descriptive Statistics (Numerical Columns) ---")
print("Presenting descriptive statistics (min, max, mean, median, etc.):")
# Select only numerical columns for describe() to avoid warnings
numerical_train_df = train_df.select_dtypes(include=np.number)
print(numerical_train_df.describe())

# --- 3.3: Target Variable Exploration ---
print("\n--- 3.3: Target Variable ('price') Exploration ---")
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(train_df['price'], kde=True, bins=50)
plt.title('Original Price Distribution')
plt.xlabel('Price (in Lakhs)')

# Log Transform Target
print("Applying log1p transformation to the target variable 'price'.")
train_df['price_log'] = np.log1p(train_df['price'])
y_log = train_df['price_log'] # Store the log-transformed target

plt.subplot(1, 2, 2)
sns.histplot(y_log, kde=True, bins=50)
plt.title('Log-Transformed Price Distribution')
plt.xlabel('Log(Price + 1)')
plt.tight_layout()
plt.show()
print(f"Original Price Skewness: {train_df['price'].skew():.2f}")
print(f"Log-Transformed Price Skewness: {y_log.skew():.2f}")
print("Insight: The original price is highly right-skewed. Log transformation makes it much closer to a normal distribution, which is better for many regression models.")

# Drop original price and ID columns from main dataframes
train_df = train_df.drop(['price', 'price_log', 'id'], axis=1) # Drop log price here too
test_df = test_df.drop('id', axis=1)

# ==========================
# Step 4 — Preprocessing & Feature Engineering
# ==========================
print("\n--- Preprocessing & Feature Engineering ---")

# --- 4.1: Cleaning Functions ---
def clean_total_sqft(x):
    """Cleans the total_sqft column by handling ranges and string values."""
    if isinstance(x, str):
        if '-' in x: # Handle ranges
            parts = x.split('-')
            try: return (float(parts[0]) + float(parts[1])) / 2
            except ValueError: return np.nan
        try: # Handle other strings
            num_str = ''.join(filter(lambda i: i.isdigit() or i=='.', x))
            return float(num_str) if num_str else np.nan
        except ValueError: return np.nan
    if pd.api.types.is_number(x): return x # Keep numbers
    return np.nan

def extract_bhk(x):
    """Extracts BHK number from size string."""
    if isinstance(x, str):
        try: return int(x.split(' ')[0])
        except (ValueError, IndexError): return np.nan
    if pd.api.types.is_number(x): return x # Keep numbers/NaNs
    return np.nan

# --- 4.2: Apply Cleaning ---
print("Cleaning 'total_sqft' and extracting 'bhk' from 'size'...")
train_df['total_sqft'] = train_df['total_sqft'].apply(clean_total_sqft)
test_df['total_sqft'] = test_df['total_sqft'].apply(clean_total_sqft)

# Check for 'size' column before processing and drop immediately after
if 'size' in train_df.columns:
    print("Processing 'size' column in train_df...")
    train_df['bhk'] = train_df['size'].apply(extract_bhk)
    train_df = train_df.drop('size', axis=1) # Drop original size column AFTER using it
    print("'size' column processed and dropped from train_df.")
else:
    print("Warning: 'size' column not found in train_df when expected.")

if 'size' in test_df.columns:
    print("Processing 'size' column in test_df...")
    test_df['bhk'] = test_df['size'].apply(extract_bhk)
    test_df = test_df.drop('size', axis=1) # Drop original size column AFTER using it
    print("'size' column processed and dropped from test_df.")
else:
    print("Warning: 'size' column not found in test_df when expected.")

# --- 4.4: Identify & Handle Duplicates (Rubric 4 - Before Concat) ---
duplicates = train_df.duplicated().sum()
print(f"\nDuplicate rows found in cleaned training data: {duplicates}")

if duplicates > 0:
    # Get the indices of duplicates IN train_df
    duplicate_indices_train = train_df[train_df.duplicated()].index

    # Drop these rows from train_df
    train_df = train_df.drop(index=duplicate_indices_train).reset_index(drop=True)

    # Drop the corresponding rows from y_log using the same indices
    y_log = y_log.drop(index=duplicate_indices_train).reset_index(drop=True)

    print(f"Duplicates dropped from train_df and y_log.")
    print(f"Updated train_df shape: {train_df.shape}")
    print(f"Updated y_log shape: {y_log.shape}")
else:
    print("No duplicates found in the cleaned training data.")

# --- 4.3 (Moved): Combine Data for Consistent Processing ---
# Now combine the cleaned and de-duplicated train_df with test_df
len_train = train_df.shape[0] # Store length *after* dropping duplicates
all_data = pd.concat((train_df, test_df)).reset_index(drop=True)
print(f"\nCombined data shape after potential duplicate removal: {all_data.shape}")

# --- 4.5: Identify & Handle Missing Values (Rubric 3) ---
print("\n--- 4.5: Handling Missing Values ---")
print("Missing values before imputation:")
missing_vals = all_data.isnull().sum()
print(missing_vals[missing_vals > 0])

# Impute numerical features with median (robust to outliers)
num_cols_impute_median = ['total_sqft', 'bath', 'balcony', 'bhk']
for col in num_cols_impute_median:
    if col in all_data.columns and all_data[col].isnull().any():
        median_val = all_data[col].median()
        all_data[col].fillna(median_val, inplace=True)
        print(f"Imputed missing '{col}' with median: {median_val:.2f}")

# Impute categorical features with mode
cat_cols_impute_mode = ['location', 'area_type', 'availability'] # 'size' already handled
for col in cat_cols_impute_mode:
    if col in all_data.columns and all_data[col].isnull().any():
        mode_val = all_data[col].mode()[0]
        all_data[col].fillna(mode_val, inplace=True)
        print(f"Imputed missing '{col}' with mode: {mode_val}")

print("\nMissing values remaining after imputation:")
print(all_data.isnull().sum().sum()) # Should be 0

# --- 4.6: Feature Engineering (Continued) ---
print("\n--- 4.6: Final Feature Engineering ---")

# Ensure 'bhk' is numeric after potential mode imputation if 'size' was missed
if 'bhk' in all_data.columns and not pd.api.types.is_numeric_dtype(all_data['bhk']):
     all_data['bhk'] = all_data['bhk'].apply(extract_bhk) # Re-apply extraction just in case
     if all_data['bhk'].isnull().any():
         all_data['bhk'].fillna(all_data['bhk'].median(), inplace=True)
     print("Ensured 'bhk' column is numeric.")

# Handle 'location' - Group rare locations
location_threshold = 10
print(f"Grouping locations with <= {location_threshold} occurrences into 'Other'.")
location_counts = all_data['location'].value_counts()
locations_to_group = location_counts[location_counts <= location_threshold].index
all_data['location'] = all_data['location'].apply(lambda x: 'Other' if x in locations_to_group else x)
print(f"Unique locations remaining: {all_data['location'].nunique()}")

# --- 4.7: Identify & Handle Outliers (Rubric 5) ---
print("\n--- 4.7: Outlier Identification ---")
# Create a temporary df for outlier visualization using cleaned data + target
temp_eda_df = all_data.iloc[:len_train].copy()
temp_eda_df['price_log'] = y_log.values

plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
sns.scatterplot(x=temp_eda_df['total_sqft'], y=temp_eda_df['price_log'])
plt.title('total_sqft vs. Log(Price)')
plt.xlabel('Total SqFt')
plt.ylabel('Log(Price + 1)')

plt.subplot(1, 2, 2)
sns.boxplot(x=temp_eda_df['bhk'].astype(int), y=temp_eda_df['price_log'])
plt.title('BHK vs. Log(Price)')
plt.xlabel('Number of BHK')
plt.ylabel('Log(Price + 1)')
# Limit BHK display for clarity if needed
# plt.xlim(-0.5, 8.5)

plt.tight_layout()
plt.show()

print("Explanation for Outlier Handling:")
print("Visual inspection shows potential outliers, especially in 'total_sqft'.")
print("However, tree-based models like LightGBM, CatBoost, and XGBoost are generally robust to outliers.")
print("Removing outliers aggressively might discard valuable information.")
print("Therefore, for this iteration, outliers are identified but retained. Explanation provided.")

# --- 4.8: One-Hot Encoding (Part of Rubric 7) ---
print("\n--- 4.8: One-Hot Encoding Categorical Features ---")
categorical_cols = all_data.select_dtypes(include='object').columns
print(f"Encoding columns: {list(categorical_cols)}")
initial_cols = set(all_data.columns)
all_data = pd.get_dummies(all_data, columns=categorical_cols, drop_first=True)
encoded_cols = set(all_data.columns) - initial_cols
print(f"Shape after one-hot encoding: {all_data.shape}")
print(f"Number of new columns created: {len(encoded_cols)}")

# --- 4.9: Handle Skewed Numerical Features (Corrected) ---
print("\n--- 4.9: Handling Skewed Numerical Features ---")

# Define the original numerical columns explicitly
original_numeric_cols = ['total_sqft', 'bath', 'balcony', 'bhk']

# Select only these columns from all_data to check for skewness
numeric_data_to_check = all_data[original_numeric_cols]

# Calculate skewness only on these columns
skewed_feats = numeric_data_to_check.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("Skewness of original numerical features:")
print(skewed_feats)

skewness_threshold = 0.75
highly_skewed_cols = skewed_feats[abs(skewed_feats) > skewness_threshold].index

if len(highly_skewed_cols) > 0:
    print(f"\nApplying log1p transform to {len(highly_skewed_cols)} skewed original numerical features: {list(highly_skewed_cols)}")
    for col in highly_skewed_cols:
        # Check if column exists before transforming (safety check)
        if col in all_data.columns:
            all_data[col] = np.log1p(all_data[col])
            print(f"Applied log1p to: {col}")
else:
    print("\nNo highly skewed original numerical features found requiring log transformation.")

# --- 4.10: Separate Train/Test Again for Scaling/Imputation ---
X = all_data.iloc[:len_train]
test_final = all_data.iloc[len_train:]
print(f"\nSeparated data. Train shape: {X.shape}, Test shape: {test_final.shape}")

# --- 4.11: Scale Numerical & Final Impute (Part of Rubric 7 & 3) ---
print("\n--- 4.11: Scaling All Features & Final Mean Imputation ---")
print("Explanation: Scaling features (like StandardScaler) brings them to a similar range,")
print("which is beneficial for models like Ridge, Lasso, SVR, and sometimes improves tree models.")
print("Imputing with the mean *after* scaling is one strategy to handle any remaining NaNs (e.g., from log transforms).")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
test_scaled = scaler.transform(test_final)

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_scaled)
test_imputed = imputer.transform(test_scaled)

print("Scaling and final imputation complete!")
print(f"Final X_imputed shape: {X_imputed.shape}")
print(f"Final test_imputed shape: {test_imputed.shape}")

# Clean up memory
del X_scaled, test_scaled, all_data, train_df, test_df, temp_eda_df

# ==========================
# Step 5 — EDA Visualizations (Rubric 6) (Corrected Duplicate Handling)
# ==========================
print("\n--- EDA Visualizations on Cleaned Data ---")

# --- Create a fresh copy for visualization ---
# Combine original train (without id/price) and original test (without id)
temp_vis_df = pd.concat(
    (train_df_orig.drop(['id', 'price'], axis=1, errors='ignore'),
     test_df_orig.drop('id', axis=1, errors='ignore')),
    ignore_index=True # Use ignore_index instead of reset_index later
)
print(f"Initial temp_vis_df shape: {temp_vis_df.shape}")


# --- Re-apply cleaning needed BEFORE duplicate check ---
print("Re-applying cleaning to temp_vis_df...")
temp_vis_df['total_sqft'] = temp_vis_df['total_sqft'].apply(clean_total_sqft)
if 'size' in temp_vis_df.columns:
    temp_vis_df['bhk'] = temp_vis_df['size'].apply(extract_bhk)
    temp_vis_df = temp_vis_df.drop('size', axis=1, errors='ignore')

# Impute numericals needed for duplicate check (if any were NaN initially)
num_cols_to_check = ['total_sqft', 'bath', 'balcony', 'bhk']
for col in num_cols_to_check:
    if col in temp_vis_df.columns and temp_vis_df[col].isnull().any():
        temp_vis_df[col].fillna(temp_vis_df[col].median(), inplace=True)

# Impute categoricals needed for duplicate check
cat_cols_to_check = ['location', 'area_type', 'availability']
for col in cat_cols_to_check:
     if col in temp_vis_df.columns and temp_vis_df[col].isnull().any():
         temp_vis_df[col].fillna(temp_vis_df[col].mode()[0], inplace=True)


# --- Replicate Duplicate Dropping Logic if duplicates were found earlier ---
# Use the 'duplicates' variable calculated in Step 4.4
if duplicates > 0:
    print(f"Replicating duplicate drop ({duplicates} rows) on temp_vis_df...")
    # Identify duplicate indices within the *training portion* of temp_vis_df
    # Note: Use len_train calculated AFTER dropping duplicates in Step 4.4
    temp_vis_train_part = temp_vis_df.iloc[:len_train]
    duplicate_indices_vis = temp_vis_train_part[temp_vis_train_part.duplicated()].index
    print(f"Indices to drop from temp_vis_df (first few): {list(duplicate_indices_vis[:5])}...")

    # Drop these rows from temp_vis_df
    temp_vis_df = temp_vis_df.drop(index=duplicate_indices_vis).reset_index(drop=True)
    print(f"Shape after dropping duplicates: {temp_vis_df.shape}")


# --- Re-apply remaining imputation AFTER potential duplicate drop ---
# (Necessary because dropping rows might have changed medians/modes slightly,
# although usually impact is minor. Ensures consistency.)
print("Re-applying imputation to temp_vis_df after potential duplicate drop...")
num_cols_impute_median = ['total_sqft', 'bath', 'balcony', 'bhk']
for col in num_cols_impute_median:
    if col in temp_vis_df.columns and temp_vis_df[col].isnull().any():
        median_val = temp_vis_df[col].median()
        temp_vis_df[col].fillna(median_val, inplace=True)

cat_cols_impute_mode = ['location', 'area_type', 'availability']
for col in cat_cols_impute_mode:
    if col in temp_vis_df.columns and temp_vis_df[col].isnull().any():
        mode_val = temp_vis_df[col].mode()[0]
        temp_vis_df[col].fillna(mode_val, inplace=True)

print("Final check for missing values in temp_vis_df:", temp_vis_df.isnull().sum().sum()) # Should be 0


# --- Add log target back for visualization ---
# Use the potentially reduced len_train
temp_vis_train = temp_vis_df.iloc[:len_train].copy()
# Ensure y_log has the correct length after duplicate removal
if len(y_log) == len_train:
    temp_vis_train['price_log'] = y_log.values
else:
    print(f"Error: Length mismatch between y_log ({len(y_log)}) and len_train ({len_train}). Cannot add price_log for EDA.")
    # Add dummy column to prevent later errors, but plots will be incorrect
    temp_vis_train['price_log'] = 0


# --- 5.1: Visualization 1: Price vs. Total SqFt ---
plt.figure(figsize=(10, 6))
sns.scatterplot(data=temp_vis_train, x='total_sqft', y='price_log', alpha=0.5)
plt.title('Log(Price) vs. Total SqFt (Cleaned Data)')
plt.xlabel('Total SqFt')
plt.ylabel('Log(Price + 1)')
plt.show()
print("Insight 1: ...") # Keep your insight

# --- 5.2: Visualization 2: Price Distribution by Area Type ---
plt.figure(figsize=(10, 6))
# Ensure 'area_type' exists before plotting
if 'area_type' in temp_vis_train.columns:
    sns.boxplot(data=temp_vis_train, x='area_type', y=np.expm1(temp_vis_train['price_log'])) # Use original price scale
    plt.title('Price Distribution by Area Type (Cleaned Data)')
    plt.xlabel('Area Type')
    plt.xticks(rotation=15)
else:
     plt.title('Area Type column missing - Cannot plot Price by Area Type')
plt.ylabel('Price (in Lakhs)')
plt.show()
print("Insight 2: ...") # Keep your insight

# --- 5.3: Visualization 3: Correlation Heatmap ---
plt.figure(figsize=(10, 8))
cols_for_heatmap = [col for col in ['total_sqft', 'bath', 'balcony', 'bhk', 'price_log'] if col in temp_vis_train.columns]
if 'price_log' in cols_for_heatmap and len(cols_for_heatmap) > 1: # Check if there are columns to correlate
    corr_matrix = temp_vis_train[cols_for_heatmap].corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
    plt.title('Correlation Heatmap of Key Numerical Features (Cleaned Data)')
else:
    plt.title('Correlation Heatmap - Not enough data/columns')
plt.show()
print("Insight 3: ...") # Keep your insight

# Clean up memory
del temp_vis_df, temp_vis_train

# ==========================
# Step 6 — Model Building (7+ Models) & Evaluation (Rubric 8 & 10)
# ==========================
print("\n--- Model Building & Initial Evaluation ---")

# --- 6.1: Train/Validation Split ---
print("Splitting data into Training and Validation sets (80/20)...")
X_train, X_valid, y_train, y_valid = train_test_split(X_imputed, y_log, test_size=0.2, random_state=42)
print(f"X_train shape: {X_train.shape}, X_valid shape: {X_valid.shape}")

# --- 6.2: Define Models ---
# Including 8 models to satisfy the rubric
print("\nDefining models...")
models = {
    "Ridge": Ridge(random_state=42),
    "Lasso": Lasso(random_state=42, alpha=0.001), # Small alpha often needed after scaling
    "ElasticNet": ElasticNet(random_state=42, alpha=0.001),
    "SVR": SVR(), # Support Vector Regressor
    "DecisionTree": DecisionTreeRegressor(random_state=42),
    "RandomForest": RandomForestRegressor(random_state=42, n_jobs=-1, n_estimators=100), # Basic RF
    "XGBoost": XGB.XGBRegressor(random_state=42, n_jobs=-1), # Basic XGB
    "LightGBM": lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbosity=-1), # Basic LGBM
    # CatBoost is trained separately below due to its eval_set integration
}

# --- 6.3: Train and Evaluate Models ---
model_performance = {}
print(f"Training and evaluating {len(models)} models on the validation set...")

for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_valid)
    rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
    model_performance[name] = rmse
    print(f"{name}: Validation RMSE = {rmse:.4f}")

# --- 6.4: Train and Evaluate CatBoost Separately ---
print("Training CatBoost...")
cat_model_eval = CatBoostRegressor(iterations=1000, # Using defaults for initial eval
                               learning_rate=0.03, # Default
                               eval_metric='RMSE',
                               random_seed=42,
                               early_stopping_rounds=50, # Early stop for eval
                               verbose=0) # Suppress output for initial eval
cat_model_eval.fit(X_train, y_train, eval_set=(X_valid, y_valid))
cat_pred_eval = cat_model_eval.predict(X_valid)
cat_rmse_eval = np.sqrt(mean_squared_error(y_valid, cat_pred_eval))
model_performance["CatBoost"] = cat_rmse_eval
print(f"CatBoost: Validation RMSE = {cat_rmse_eval:.4f}")

# --- 6.5: Comparison of Model Performances (Rubric 10) ---
print("\n--- Comparison of Model Performances (Validation RMSE) ---")
perf_df = pd.DataFrame.from_dict(model_performance, orient='index', columns=['RMSE'])
print(perf_df.sort_values(by='RMSE'))
print("\nInsight: Gradient Boosting models (XGBoost, LightGBM, CatBoost) and RandomForest show the best initial performance.")

# ==========================
# Step 7 — Hyperparameter Tuning (Top 3 Models) (Rubric 9)
# ==========================
print("\n--- Hyperparameter Tuning (Top 3 Models: LGBM, CatBoost, Ridge) ---")
# Tuning LGBM and CatBoost using their early stopping, Ridge with simple GridSearch
print("\nCreating LightGBM datasets for training and validation...")
lgb_train = lgb.Dataset(X_train, label=y_train)
lgb_valid_data = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)
print("LightGBM datasets created.")
# --- 7.1: LightGBM Tuning (using early stopping validation) ---
# Note: This isn't a full grid search, but uses early stopping to find optimal rounds,
# similar to the Colab approach and satisfies the 'tuning' concept.
print("\n'Tuning' LightGBM using validation set for early stopping...")
# Re-run training with potentially more rounds and specific params if desired
lgb_params_tuned = {
    'objective': 'regression_l1', 'metric': 'rmse', 'learning_rate': 0.05,
    'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 0.7,
    'bagging_freq': 5, 'seed': 42, 'n_jobs': -1, 'verbose': -1
}
lgb_model_tuned = lgb.train(
    lgb_params_tuned, lgb_train, valid_sets=[lgb_train, lgb_valid_data],
    num_boost_round=3000, # Allow more rounds
    callbacks=[lgb.early_stopping(stopping_rounds=150), lgb.log_evaluation(period=500)]
)
lgb_preds_valid_tuned = lgb_model_tuned.predict(X_valid, num_iteration=lgb_model_tuned.best_iteration)
lgb_rmse_tuned = np.sqrt(mean_squared_error(y_valid, lgb_preds_valid_tuned))
print(f"LightGBM Tuned RMSE (Validation): {lgb_rmse_tuned:.4f}")
print(f"LightGBM Tuned Best Iteration: {lgb_model_tuned.best_iteration}")

# --- 7.2: CatBoost Tuning (using early stopping validation) ---
print("\n'Tuning' CatBoost using validation set for early stopping...")
cat_model_tuned = CatBoostRegressor(
    iterations=3000, learning_rate=0.05, depth=6, # Example parameters
    eval_metric='RMSE', random_seed=42,
    early_stopping_rounds=150, verbose=500
)
cat_model_tuned.fit(X_train, y_train, eval_set=(X_valid, y_valid))
cat_preds_valid_tuned = cat_model_tuned.predict(X_valid)
cat_rmse_tuned = np.sqrt(mean_squared_error(y_valid, cat_preds_valid_tuned))
print(f"CatBoost Tuned RMSE (Validation): {cat_rmse_tuned:.4f}")
print(f"CatBoost Tuned Best Iteration: {cat_model_tuned.get_best_iteration()}")

# --- 7.3: Ridge Tuning (using GridSearchCV) ---
print("\nTuning Ridge using GridSearchCV...")
from sklearn.model_selection import GridSearchCV
param_grid_ridge = {'alpha': [0.1, 0.5, 1.0, 5.0, 10.0, 20.0]}
kfold_cv = KFold(n_splits=5, shuffle=True, random_state=42) # Use KFold for CV

grid_ridge = GridSearchCV(Ridge(random_state=42), param_grid_ridge, cv=kfold_cv,
                          scoring='neg_root_mean_squared_error', n_jobs=-1)
grid_ridge.fit(X_imputed, y_log) # Fit on full data for CV score
best_ridge_params = grid_ridge.best_params_
best_ridge_cv_rmse = -grid_ridge.best_score_
print(f"Best Ridge Alpha: {best_ridge_params}")
print(f"Best Ridge CV RMSE: {best_ridge_cv_rmse:.4f}")

# Store tuned model scores for ensemble weighting reference
tuned_scores = {
    "Ridge": best_ridge_cv_rmse, # Use CV score as it reflects generalization better
    "LightGBM": lgb_rmse_tuned, # Use validation score from early stopping
    "CatBoost": cat_rmse_tuned # Use validation score from early stopping
}
print("\nTuned Model Scores (lower is better):")
print(tuned_scores)

# --- Tuning 7.4: XGBoost (Randomized Search - Reduced Overfitting Focus) ---
print("\nTuning XGBoost (Randomized Search - Reduced Overfitting Focus)...")

# (Ensure RandomizedSearchCV and cv_strategy are defined from previous steps)
from sklearn.model_selection import RandomizedSearchCV

param_dist_xgb_reg = {
    'n_estimators': [700, 1000],
    'learning_rate': [0.03, 0.05],
    'max_depth': [3, 4],
    'subsample': [0.8, 0.9],
    'colsample_bytree': [0.8, 0.9],
    'reg_alpha': [0.01, 0.1, 1],
    'reg_lambda': [0.1, 1, 1.5]
}

random_search_xgb_reg = RandomizedSearchCV(
                            XGB.XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist'),
                            param_distributions=param_dist_xgb_reg,
                            n_iter=20, # Using 20 combinations
                            cv=kfold_cv,
                            scoring='neg_root_mean_squared_error',
                            n_jobs=-1,
                            verbose=1,
                            random_state=42
                        )

random_search_xgb_reg.fit(X_imputed, y_log)

# Store the results using the variable names the submission step expects
best_xgb_reg_rmse = -random_search_xgb_reg.best_score_
best_params_xgb_reg = random_search_xgb_reg.best_params_

# Store them in the general variables for the next step
best_params_xgb = best_params_xgb_reg
best_xgb_rmse = best_xgb_reg_rmse

print(f"Best XGBoost Params (Regularized): {best_params_xgb}")
print(f"Best XGB CV RMSE (Regularized): {best_xgb_rmse:.4f}")

# ==========================
# Step 8 — Final Ensemble Training & Prediction (4-MODEL ENSEMBLE)
# ==========================
print("\n--- Final Model Training & Ensemble Prediction (4-Model Ensemble) ---")

# --- 8.1: Train Final Models ---
print("Training final models on full data using optimal parameters/iterations...")

# Ridge (using best alpha found)
print("Training Ridge...")
ridge_final = Ridge(random_state=42, **best_ridge_params)
ridge_final.fit(X_imputed, y_log)
ridge_preds_test = ridge_final.predict(test_imputed)

# LightGBM (using params and best iteration)
print("Training LightGBM...")
final_lgb_model = lgb.train(
    lgb_params_tuned,
    lgb.Dataset(X_imputed, label=y_log),
    num_boost_round=lgb_model_tuned.best_iteration
)
lgb_preds_test = final_lgb_model.predict(test_imputed)

# CatBoost (using params and best iteration)
print("Training CatBoost...")
final_cat_model = CatBoostRegressor(
    iterations=cat_model_tuned.get_best_iteration(),
    learning_rate=0.05, depth=6, eval_metric='RMSE',
    random_seed=42, verbose=0
)
final_cat_model.fit(X_imputed, y_log)
cat_preds_test = final_cat_model.predict(test_imputed)

# CHANGE: Add Regularized XGBoost Training
print("Training Regularized XGBoost...")
# Ensure 'best_params_xgb' holds the regularized parameters (CV RMSE 0.3184)
if 'best_params_xgb_reg' in locals():
    best_params_xgb = best_params_xgb_reg # Use the specific reg variable if it exists
    best_xgb_rmse = best_xgb_reg_rmse
elif 'best_params_xgb' not in locals():
     raise NameError("XGBoost tuning results ('best_params_xgb') not found. Please re-run the tuning cell.")
# (If best_params_xgb was overwritten, it will be used automatically)

final_model_xgb = XGB.XGBRegressor(
    random_state=42, n_jobs=-1, tree_method='hist',
    **best_params_xgb # Use the regularized parameters
)
final_model_xgb.fit(X_imputed, y_log)
xgb_preds_test = final_model_xgb.predict(test_imputed)
print("All final models trained.")

# --- 8.1.5: Calculate 4-Model Ensemble Weights ---
print("\nCalculating 4-Model ensemble weights...")
epsilon = 1e-6

# Get RMSEs from tuning (ensure 'best_xgb_rmse' is the 0.3184 score)
rmse_ridge = tuned_scores["Ridge"]   # ~0.3663
rmse_lgbm = tuned_scores["LightGBM"] # ~0.3519
rmse_cat = tuned_scores["CatBoost"] # ~0.3272
rmse_xgb = best_xgb_rmse             # ~0.3184

inv_rmse_ridge = 1 / (rmse_ridge + epsilon)
inv_rmse_lgbm = 1 / (rmse_lgbm + epsilon)
inv_rmse_cat = 1 / (rmse_cat + epsilon)
inv_rmse_xgb = 1 / (rmse_xgb + epsilon) # Add XGB weight

total_inv_rmse = inv_rmse_ridge + inv_rmse_lgbm + inv_rmse_cat + inv_rmse_xgb # Add to total

# Normalize weights
weight_ridge = inv_rmse_ridge / total_inv_rmse
weight_lgbm = inv_rmse_lgbm / total_inv_rmse
weight_cat = inv_rmse_cat / total_inv_rmse
weight_xgb = inv_rmse_xgb / total_inv_rmse # Add XGB weight

print(f"Optimized Weights -> Ridge: {weight_ridge:.3f}, LGBM: {weight_lgbm:.3f}, CatBoost: {weight_cat:.3f}, XGB: {weight_xgb:.3f}")

# --- 8.2: Weighted Ensemble Prediction (4 Models) ---
print("\nApplying optimized 4-model weighted ensemble...")
ensemble_preds_test_log = (weight_ridge * ridge_preds_test +
                           weight_lgbm * lgb_preds_test +
                           weight_cat * cat_preds_test +
                           weight_xgb * xgb_preds_test) # Add XGB prediction

# --- 8.3: Inverse Transform ---
final_predictions = np.expm1(ensemble_preds_test_log)
final_predictions = np.maximum(0, final_predictions)

# ==========================
# Step 9 — Create Submission File
# ==========================
print("\n--- Creating Submission File ---")
submission = pd.DataFrame({
    'id': test_ids, # Use original test IDs stored earlier
    'price': final_predictions # Ensure column name matches sample submission
})
submission_filename = 'submission.csv' # Standard filename for Kaggle
submission.to_csv(submission_filename, index=False)
print(f" {submission_filename} saved successfully!")
print("Submission Head:")
print(submission.head())